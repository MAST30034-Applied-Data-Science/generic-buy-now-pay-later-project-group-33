{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = []\n",
    "period1 = pd.date_range('2021-2-28','2021-08-27', freq='D').strftime(\"%Y-%m-%d\").tolist()\n",
    "period2 = pd.date_range('2021-8-28','2022-02-27', freq='D').strftime(\"%Y-%m-%d\").tolist()\n",
    "period3 = pd.date_range('2022-2-28','2022-08-28', freq='D').strftime(\"%Y-%m-%d\").tolist()\n",
    "path_prefix1 = '../data/tables/transactions_20210228_20210827_snapshot/order_datetime='\n",
    "path_prefix2 = '../data/tables/transactions_20210828_20220227_snapshot/order_datetime='\n",
    "path_prefix3 = '../data/tables/transactions_20220228_20220828_snapshot/order_datetime='\n",
    "##for date in period1:\n",
    "##    path_list.append(path_prefix1 + date)\n",
    "##for date in period2:\n",
    "##   path_list.append(path_prefix2 + date)\n",
    "##for date in period3:\n",
    "##    path_list.append(path_prefix3 + date)\n",
    "\n",
    "##raw_transactions_df = spark.read.parquet(*path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------------+--------------------+\n",
      "|user_id|merchant_abn|      dollar_value|            order_id|\n",
      "+-------+------------+------------------+--------------------+\n",
      "|  14935| 79417999332|136.06570809815838|23acbb7b-cf98-458...|\n",
      "|      1| 46451548968| 72.61581642788431|76bab304-fa2d-400...|\n",
      "|  14936| 89518629617|3.0783487174439297|a2ae446a-2959-41c...|\n",
      "|      1| 49167531725| 51.58228625503599|7080c274-17f7-4cc...|\n",
      "|  14936| 31101120643|25.228114942417797|8e301c0f-06ab-45c...|\n",
      "+-------+------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##raw_transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of date\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('user_id', LongType(), True), StructField('merchant_abn', LongType(), True), StructField('dollar_value', DoubleType(), True), StructField('order_id', StringType(), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSchema = spark.read.parquet('../data/tables/transactions_20210228_20210827_snapshot/order_datetime=2021-02-28')\n",
    "dfSchema.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>merchant_abn</th><th>dollar_value</th><th>order_id</th><th>date</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------------+------------+--------+----+\n",
       "|user_id|merchant_abn|dollar_value|order_id|date|\n",
       "+-------+------------+------------+--------+----+\n",
       "+-------+------------+------------+--------+----+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a empty dataframe to \n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, LongType, DoubleType, IntegerType\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "schema = StructType([StructField('user_id', LongType(), True), StructField('merchant_abn', LongType(), True), StructField('dollar_value', DoubleType(), True), StructField('order_id', StringType(), True), StructField('date', IntegerType(), False)])\n",
    "df = spark.createDataFrame(emptyRDD,schema)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "##period4 = pd.date_range('2021-2-28','2021-03-27', freq='D').strftime(\"%Y-%m-%d\").tolist()\n",
    "##for date in period4:\n",
    "\n",
    "##   tep = spark.read.parquet(path_prefix1 + date)\n",
    "    \n",
    "##   tep = tep.withColumn(\"dateInInt\", lit(date))\n",
    "##   df = df.union(tep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of +-------+------------+------------------+--------------------+----------+\n",
       "|user_id|merchant_abn|      dollar_value|            order_id|      date|\n",
       "+-------+------------+------------------+--------------------+----------+\n",
       "|      1| 28000487688|133.22689421562643|0c37b3f7-c7f1-48c...|2021-02-28|\n",
       "|  18485| 62191208634| 79.13140006851712|9e18b913-0465-4fd...|2021-02-28|\n",
       "|      1| 83690644458|30.441348317517228|40a2ff69-ea34-465...|2021-02-28|\n",
       "|  18488| 39649557865| 962.8133405407585|f4c1a5ae-5b76-40d...|2021-02-28|\n",
       "|      2| 80779820715| 48.12397733548124|cd09bdd6-f56d-489...|2021-02-28|\n",
       "|  18489| 43186523025| 98.14878546968934|9008a98e-1b02-4de...|2021-02-28|\n",
       "|      3| 29566626791| 46.33087226118639|26b7574e-81c2-455...|2021-02-28|\n",
       "|  18490| 93558142492|232.83335268750145|2bda0665-796f-4f2...|2021-02-28|\n",
       "|      3| 32361057556| 87.34942171371054|633a7656-2fcc-4b8...|2021-02-28|\n",
       "|  18491| 64974914166|130.12601873970038|4bc15338-83eb-43d...|2021-02-28|\n",
       "|      5| 83177825742| 66.66426160206629|43e3b3fe-791b-47f...|2021-02-28|\n",
       "|  18493| 43186523025| 69.75645555605993|6c494428-b293-459...|2021-02-28|\n",
       "|      6| 21772962346|113.04510652600983|000ca5ac-3247-4ef...|2021-02-28|\n",
       "|  18493| 67979471799| 53.03432496202065|dd61feff-5aa3-43d...|2021-02-28|\n",
       "|      7| 66079287213| 89.93535555889761|50a5619d-0647-487...|2021-02-28|\n",
       "|  18494| 49891706470|21.995368431723854|faac27e9-c054-44d...|2021-02-28|\n",
       "|     11| 98269572896|129.46280909485031|8f5d0cab-8055-435...|2021-02-28|\n",
       "|  18495| 66667026714|  48.3228941934147|ca150cbf-6e34-489...|2021-02-28|\n",
       "|     11| 20692490685|196.93916081228323|09bc8dd6-419f-4cb...|2021-02-28|\n",
       "|  18496| 60956456424| 82.30827454121753|d9df2270-6fb9-46e...|2021-02-28|\n",
       "+-------+------------+------------------+--------------------+----------+\n",
       "only showing top 20 rows\n",
       ">"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for date in period1:\n",
    "\n",
    "    tep = spark.read.parquet(path_prefix1 + date)\n",
    "    \n",
    "    tep = tep.withColumn(\"date\", lit(date))\n",
    "    df = df.union(tep)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in period2:\n",
    "\n",
    "    tep = spark.read.parquet(path_prefix2 + date)\n",
    "    \n",
    "    tep = tep.withColumn(\"date\", lit(date))\n",
    "    df = df.union(tep)\n",
    "\n",
    "for date in period3:\n",
    "\n",
    "    tep = spark.read.parquet(path_prefix3 + date)\n",
    "    \n",
    "    tep = tep.withColumn(\"date\", lit(date))\n",
    "    df = df.union(tep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
